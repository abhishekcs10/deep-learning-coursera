{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coursera_nn_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekcs10/deep-learning-coursera/blob/master/Coursera_nn_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5iXHp1oKHUH",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Building basic functions with numpy ##\n",
        "\n",
        "Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape. You will need to know how to use these functions for future assignments.\n",
        "\n",
        "### 1.1 - sigmoid function, np.exp() ###\n",
        "\n",
        "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().\n",
        "\n",
        "**Exercise**: Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.\n",
        "\n",
        "**Reminder**:\n",
        "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
        "\n",
        "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
        "\n",
        "To refer to a function belonging to a specific package you could call it using package_name.function(). Run the code below to see an example with math.exp()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9J-0Z_iJztx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "#X is a scaler numpy vector\n",
        "def my_sigmoid(X): \n",
        "  ans=1.0/(1+math.exp(-X))\n",
        "  return ans\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrz-W38KGh1",
        "colab_type": "text"
      },
      "source": [
        "Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1esI6up_L2fm",
        "colab_type": "code",
        "outputId": "b8fea63e-568a-4983-d1c5-8ed14267d346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "my_sigmoid(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9525741268224334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awJY76vDNh1x",
        "colab_type": "text"
      },
      "source": [
        "This will give give error because math library deals with a single entity. This is where numpy is useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpcQgws2L5pP",
        "colab_type": "code",
        "outputId": "0cc577ad-2029-4a19-80d8-ec5726f8acd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "X=[1,2,3]\n",
        "my_sigmoid(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fea03189e244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-c1adf41fc050>\u001b[0m in \u001b[0;36mmy_sigmoid\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#X is a scaler numpy vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nvNZG3HNdp3",
        "colab_type": "text"
      },
      "source": [
        "Here You pass a vector to a numpy array function which converts it to a numpy array.\n",
        "Then you apply numpy math functions such as exp to compute exponent of a numpy vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pikjBPDfMPG_",
        "colab_type": "code",
        "outputId": "0cc3af51-82e8-4939-b467-ea1f37627f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X=np.array([1,2,3])\n",
        "print(np.exp(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.71828183  7.3890561  20.08553692]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV2fVlGfOBHo",
        "colab_type": "text"
      },
      "source": [
        "Any time you need more info on a numpy function, we encourage you to look at [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
        "\n",
        "You can also create a new cell in the notebook and write `np.exp?` (for example) to get quick access to the documentation.\n",
        "\n",
        "**Exercise**: Implement the sigmoid function using numpy. \n",
        "\n",
        "**Instructions**: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays. You don't need to know more for now.\n",
        "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
        "    x_1  \\\\\n",
        "    x_2  \\\\\n",
        "    ...  \\\\\n",
        "    x_n  \\\\\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
        "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
        "    ...  \\\\\n",
        "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
        "\\end{pmatrix}\\tag{1} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0WRGa9XMsLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(X):\n",
        "  \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "  \"\"\"\n",
        "  ans=1.0/(1+np.exp(-X));\n",
        "  return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8SrdHqlOUDV",
        "colab_type": "code",
        "outputId": "de35f0b9-eefb-4c62-a252-88e8d2c9b8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X=np.array([1,2,3])\n",
        "sigmoid(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.88079708, 0.95257413])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxxuxLxnO0Fb",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 - Sigmoid gradient\n",
        "\n",
        "As you've seen in lecture, you will need to compute gradients to optimize loss functions using backpropagation. Let's code your first gradient function.\n",
        "\n",
        "**Exercise**: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
        "where $\\sigma(x)$ is sigmoid(x)\n",
        "\n",
        "You often code this function in two steps:\n",
        "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
        "2. Compute $\\sigma'(x) = s(1-s)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZHnoIUDOa3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_derivative(X):\n",
        "  ans=sigmoid(X)\n",
        "  return np.multiply(ans,1-ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qloG-SPJRgr8",
        "colab_type": "text"
      },
      "source": [
        "When to use what?\n",
        "\n",
        "np.dot() takes dot product of two matrices defined as follows\n",
        "\n",
        "|A B| . |E F| = |A*E+B*G A*F+B*H| \\\\\n",
        "|C D|   |G H|  |C*E+D*G C*F+D*H|\n",
        "\n",
        "Whereas np.multiply does an element-wise multiplication of two matrices and so does * .\n",
        "\n",
        "|A B| ⊙ |E F| = |A*E B*F| \\\\\n",
        "|C D|   |G H|   |C*G D*H|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaEw_gIWP3LG",
        "colab_type": "code",
        "outputId": "c8e80e0c-a8c9-40dd-d5a8-de0f889a24e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x = np.array([1, 2, 3])\n",
        "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hmF5lgttd2F",
        "colab_type": "text"
      },
      "source": [
        "# How is np.dot and np.matmul different?\n",
        "\n",
        "## Explanation by example\n",
        "\n",
        "if there is a 4x4 matrix which has to perform matmul or dot product of any other n-dimentional vector (lets say axbxc), then matmul will compute multiplcation as =>\\\\\n",
        "((4x4) matmul (axb)) repeated c times\n",
        "\n",
        "dot will compute product as=>\n",
        "((4x4)dot(axc)) repeated b times\n",
        "\n",
        "\n",
        "# numpy.dot(a, b, out=None)\n",
        "\n",
        "Dot product of two arrays. Specifically,\n",
        "\n",
        "1. If both a and b are 1-D arrays, it is inner product of vectors (without complex conjugation).\n",
        "\n",
        "2. If both a and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred.\n",
        "\n",
        "3. If either a or b is 0-D (scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a * b is preferred.\n",
        "\n",
        "4. If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n",
        "\n",
        "5. If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b:\n",
        "\n",
        "$$dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXaXpaaOQKxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fourbyfour = np.array([\n",
        "                       [1,2,3,4],\n",
        "                       [3,2,1,4],\n",
        "                       [5,4,6,7],\n",
        "                       [11,12,13,14]\n",
        "                      ])\n",
        "\n",
        "\n",
        "twobyfourbythree = np.array([\n",
        "                             [[2,3],[11,9],[32,21],[28,17]],\n",
        "                             [[2,3],[1,9],[3,21],[28,7]],\n",
        "                             [[2,3],[1,9],[3,21],[28,7]],\n",
        "                            ])\n",
        "\n",
        "\n",
        "dot1=[[2,2,2],[11,1,1],[32,3,3],[28,28,28]]\n",
        "dot2=[[2,1,3,28],[3,9,21,7]]\n",
        "dot3=[[2,1,3,28],[3,9,21,7]]\n",
        "dot4=[[28,28,28],[17,7,7]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKcW6CkvfB9r",
        "colab_type": "code",
        "outputId": "527dc3f1-66c3-4637-eeac-30e4918d08ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "#matmul example explanation\n",
        "alag1=[[2,3],[11,9],[32,21],[28,17]]\n",
        "alag2=[[2,3],[1,9],[3,21],[28,7]]\n",
        "alag3=[[2,3],[1,9],[3,21],[28,7]]\n",
        "\n",
        "print('4x4*4x2x3 matmul:\\n {}\\n'.format(np.matmul(fourbyfour,twobyfourbythree)))\n",
        "\n",
        "print('4x4*alag1 matmul:\\n {}\\n'.format(np.matmul(fourbyfour,alag1)))\n",
        "print('4x4*alag2 matmul:\\n {}\\n'.format(np.matmul(fourbyfour,alag2)))\n",
        "print('4x4*alag3 matmul:\\n {}\\n'.format(np.matmul(fourbyfour,alag3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4x4*4x2x3 matmul:\n",
            " [[[232 152]\n",
            "  [172 116]\n",
            "  [442 296]\n",
            "  [962 652]]\n",
            "\n",
            " [[125 112]\n",
            "  [123  76]\n",
            "  [228 226]\n",
            "  [465 512]]\n",
            "\n",
            " [[125 112]\n",
            "  [123  76]\n",
            "  [228 226]\n",
            "  [465 512]]]\n",
            "\n",
            "4x4*alag1 matmul:\n",
            " [[232 152]\n",
            " [172 116]\n",
            " [442 296]\n",
            " [962 652]]\n",
            "\n",
            "4x4*alag2 matmul:\n",
            " [[125 112]\n",
            " [123  76]\n",
            " [228 226]\n",
            " [465 512]]\n",
            "\n",
            "4x4*alag3 matmul:\n",
            " [[125 112]\n",
            " [123  76]\n",
            " [228 226]\n",
            " [465 512]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqPmWdhlfGYG",
        "colab_type": "code",
        "outputId": "de0e1866-84a7-4fa4-e996-4ba03dde91d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "#dot explanation\n",
        "\n",
        "print('4x4*4x2x3 dot:\\n {}\\n'.format(np.dot(fourbyfour,twobyfourbythree)))\n",
        "print('4x4*dot1 dot:\\n {}\\n'.format(np.dot(fourbyfour,dot1)))\n",
        "print('4x4*dot2 dot:\\n {}\\n'.format(np.dot(fourbyfour,dot2)))\n",
        "print('4x4*dot3 dot:\\n {}\\n'.format(np.dot(fourbyfour,dot3)))\n",
        "print('4x4*dot4 dot:\\n {}\\n'.format(np.dot(fourbyfour,dot4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4x4*4x2x3 dot:\n",
            " [[[232 152]\n",
            "  [125 112]\n",
            "  [125 112]]\n",
            "\n",
            " [[172 116]\n",
            "  [123  76]\n",
            "  [123  76]]\n",
            "\n",
            " [[442 296]\n",
            "  [228 226]\n",
            "  [228 226]]\n",
            "\n",
            " [[962 652]\n",
            "  [465 512]\n",
            "  [465 512]]]\n",
            "\n",
            "4x4*dot1 dot:\n",
            " [[232 125 125]\n",
            " [172 123 123]\n",
            " [442 228 228]\n",
            " [962 465 465]]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c93f1eee0fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4x4*4x2x3 dot:\\n {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfourbyfour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtwobyfourbythree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4x4*dot1 dot:\\n {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfourbyfour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdot1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4x4*dot2 dot:\\n {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfourbyfour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdot2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4x4*dot3 dot:\\n {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfourbyfour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdot3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'4x4*dot4 dot:\\n {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfourbyfour\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdot4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (4,4) and (2,4) not aligned: 4 (dim 1) != 2 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1SzmDJw_PY",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 - Reshaping arrays ###\n",
        "\n",
        "Two common numpy functions used in deep learning are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
        "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
        "- X.reshape(...) is used to reshape X into some other dimension. \n",
        "\n",
        "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
        "\n",
        "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
        "\n",
        "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
        "``` python\n",
        "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
        "```\n",
        "- Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need with `image.shape[0]`, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1058aqKtmHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image2vector(image):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    image -- a numpy array of shape (length, height, depth)\n",
        "    \n",
        "    Returns:\n",
        "    v -- a vector of shape (length*height*depth, 1)\n",
        "    \"\"\"\n",
        "    vec1d=image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
        "    return vec1d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npAQMmLR0ofH",
        "colab_type": "code",
        "outputId": "e5227a6a-abea-48cf-d294-14c3db206306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "image = np.array([[[ 0.67826139,  0.29380381],\n",
        "        [ 0.90714982,  0.52835647],\n",
        "        [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "       [[ 0.92814219,  0.96677647],\n",
        "        [ 0.85304703,  0.52351845],\n",
        "        [ 0.19981397,  0.27417313]],\n",
        "\n",
        "       [[ 0.60659855,  0.00533165],\n",
        "        [ 0.10820313,  0.49978937],\n",
        "        [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print (\"image2vector(image) = \" + str(image2vector(image)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image2vector(image) = [[0.67826139]\n",
            " [0.29380381]\n",
            " [0.90714982]\n",
            " [0.52835647]\n",
            " [0.4215251 ]\n",
            " [0.45017551]\n",
            " [0.92814219]\n",
            " [0.96677647]\n",
            " [0.85304703]\n",
            " [0.52351845]\n",
            " [0.19981397]\n",
            " [0.27417313]\n",
            " [0.60659855]\n",
            " [0.00533165]\n",
            " [0.10820313]\n",
            " [0.49978937]\n",
            " [0.34144279]\n",
            " [0.94630077]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY_o1TGp06vK",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 - Normalizing rows\n",
        "\n",
        "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
        "\n",
        "For example, if $$x = \n",
        "\\begin{bmatrix}\n",
        "    0 & 3 & 4 \\\\\n",
        "    2 & 6 & 4 \\\\\n",
        "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
        "    5 \\\\\n",
        "    \\sqrt{56} \\\\\n",
        "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
        "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
        "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
        "\\end{bmatrix}\\tag{5}$$ Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it in part 5.\n",
        "\n",
        "\n",
        "**Exercise**: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP2Ct7wN0wrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeRows(x):\n",
        "    \"\"\"\n",
        "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
        "    \n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (n, m)\n",
        "    \n",
        "    Returns:\n",
        "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
        "    \"\"\"\n",
        "    return x/np.linalg.norm(x,axis=1,keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rf2ajqW1S2I",
        "colab_type": "code",
        "outputId": "36e3ea67-e631-4cab-e54e-a0f1a08e750b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x = np.array([\n",
        "    [0, 3, 4],\n",
        "    [1, 6, 4]])\n",
        "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8uVHNX2UwZ",
        "colab_type": "text"
      },
      "source": [
        "**Note**:\n",
        "In normalizeRows(), you can try to print the shapes of x_norm and x, and then rerun the assessment. You'll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting and we'll talk about it now! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOSLawua2iY1",
        "colab_type": "text"
      },
      "source": [
        "### 1.5 - Broadcasting and the softmax function ####\n",
        "A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
        "\n",
        "**Exercise**: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.\n",
        "\n",
        "**Instructions**:\n",
        "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
        "    x_1  &&\n",
        "    x_2 &&\n",
        "    ...  &&\n",
        "    x_n  \n",
        "\\end{bmatrix}) = \\begin{bmatrix}\n",
        "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
        "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
        "    ...  &&\n",
        "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
        "\\end{bmatrix} $ \n",
        "\n",
        "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
        "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
        "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
        "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
        "\\end{bmatrix} = \\begin{pmatrix}\n",
        "    softmax\\text{(first row of x)}  \\\\\n",
        "    softmax\\text{(second row of x)} \\\\\n",
        "    ...  \\\\\n",
        "    softmax\\text{(last row of x)} \\\\\n",
        "\\end{pmatrix} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiVUVkK2DXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Calculates the softmax for each row of the input x.\n",
        "\n",
        "    Your code should work for a row vector and also for matrices of shape (n, m).\n",
        "\n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (n,m)\n",
        "\n",
        "    Returns:\n",
        "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
        "    \"\"\"\n",
        "    soft=np.exp(x)\n",
        "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
        "    row_sum=np.sum(soft,axis=1,keepdims=True)\n",
        "    return soft/row_sum;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAp06UY53ybB",
        "colab_type": "code",
        "outputId": "0e3f091c-be26-4cfc-eb08-d0a4a9608bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "x = np.array([\n",
        "    [9, 2, 5, 0, 0],\n",
        "    [7, 5, 0, 0 ,0]])\n",
        "print(\"softmax(x) = \" + str(softmax(x)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
            "  1.21052389e-04]\n",
            " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
            "  8.01252314e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkTdCa2ocahF",
        "colab_type": "text"
      },
      "source": [
        "**Note**:\n",
        "- If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). **x_exp/x_sum** works due to python broadcasting.\n",
        "​\n",
        "Congratulations! You now have a pretty good understanding of python numpy and have implemented a few useful functions that you will be using in deep learning.\n",
        "\n",
        "\n",
        "<font color='red'>  \n",
        "**What you need to remember:**\n",
        "- np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
        "- the sigmoid function and its gradient\n",
        "- image2vector is commonly used in deep learning\n",
        "- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. \n",
        "- numpy has efficient built-in functions\n",
        "- broadcasting is extremely useful  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLnzeExOg_SY",
        "colab_type": "text"
      },
      "source": [
        "# Difference between a[i][j] and a[i,j] in numpy\n",
        "\n",
        "You should almost always use [i, j] instead of [i][j] when dealing with numpy arrays. In many cases there's no real difference but in your case there is.\n",
        "\n",
        "Suppose you have an array like this:\n",
        "\n",
        "\n",
        "\\>>> import numpy as np\n",
        "\n",
        "\\>>> arr = np.arange(16).reshape(4, 4)\n",
        "\n",
        "\\>>> arr \n",
        "\n",
        "\n",
        "array([[ 0,  1,  2,  3],\\\n",
        "       [ 4,  5,  6,  7],\\\n",
        "       [ 8,  9, 10, 11],\\\n",
        "       [12, 13, 14, 15]])\n",
        "\n",
        "When you use [:] that's equivalent to a new view, but if you do [1, :] or [:, 1] it means get the second row (column). Roughly speaking it means: index the dimension where you had the number and leave the dimension where you had the : alone:\n",
        "\n",
        "\\>>> arr[:]\\\n",
        "array([[ 0,  1,  2,  3],\\\n",
        "       [ 4,  5,  6,  7],\\\n",
        "       [ 8,  9, 10, 11],\\\n",
        "       [12, 13, 14, 15]])\n",
        "\n",
        "\\>>> arr[:, 1]  #  get the second column\\\n",
        "array([ 1,  5,  9, 13])\\\n",
        "\\>>> arr[:][1]  # get a new view of the array, then get the second row\\\n",
        "array([4, 5, 6, 7])\\\n",
        "This is because [1] is interpreted as [1, ...] (... is the Ellipsis object) and for 2D it's equivalent to [1, :].\n",
        "\n",
        "That's also the reason why the row indexing still works (because it's the first dimension):\n",
        "\n",
        "\\>>> arr[1, :]  # get the second row\\\n",
        "array([4, 5, 6, 7])\\\n",
        "\\>>> arr[1][:]  # get the second row, then get a new view of that row\\\n",
        "array([4, 5, 6, 7])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO6XPX72iJOc",
        "colab_type": "text"
      },
      "source": [
        "## 2) Vectorization\n",
        "\n",
        "\n",
        "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjMQJpIS32zG",
        "colab_type": "code",
        "outputId": "02e4361c-5b20-4943-9526-3613fa278420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "import time\n",
        "\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "dot = 0\n",
        "for i in range(len(x1)):\n",
        "  dot=x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "out=np.zeros((len(x1),len(x2)))\n",
        "for i in range(len(x1)):\n",
        "  for j in range(len(x2)):\n",
        "    out[i,j]=x1[i]*x2[j]\n",
        "    \n",
        "toc = time.process_time()\n",
        "print (\"outer = \" + str(out) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "mul = np.zeros(len(x1))\n",
        "for i in range(len(x1)):\n",
        "    mul[i] = x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
        "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
        "tic = time.process_time()\n",
        "gdot = np.zeros(W.shape[0])\n",
        "for i in range(W.shape[0]):\n",
        "    for j in range(len(x1)):\n",
        "        gdot[i] += W[i,j]*x1[j]\n",
        "toc = time.process_time()\n",
        "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot = 0\n",
            " ----- Computation time = 0.0727629999999202ms\n",
            "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            " ----- Computation time = 0.2350350000002166ms\n",
            "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
            " ----- Computation time = 0.1705369999998041ms\n",
            "gdot = [24.58519261 18.52018066 19.99011029]\n",
            " ----- Computation time = 0.19780800000024357ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAnKgF7Yco0j",
        "colab_type": "text"
      },
      "source": [
        "# np.random.rand\n",
        "Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\n",
        "\n",
        "## numpy.random.rand(d0, d1, ..., dn)\n",
        "Random values in a given shape.\n",
        "\n",
        "Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\n",
        "\n",
        "### Parameters:\t\n",
        "d0, d1, …, dn : int, optional\n",
        "The dimensions of the returned array, should all be positive. If no argument is given a single Python float is returned.\n",
        "\n",
        "### Returns:\t\n",
        "out : ndarray, shape (d0, d1, ..., dn)\n",
        "Random values.\n",
        "\n",
        "# numpy.random.randint\n",
        "\n",
        "Return random integers from the “discrete uniform” distribution of the specified dtype in the “half-open” interval [low, high). If high is None (the default), then results are from [0, low).\n",
        "\n",
        "## numpy.random.randint(low, high=None, size=None, dtype='l')\n",
        "\n",
        "Return random integers from low (inclusive) to high (exclusive).\n",
        "\n",
        "\n",
        "### Parameters:\t\n",
        "low : int\n",
        "Lowest (signed) integer to be drawn from the distribution (unless high=None, in which case this parameter is one above the highest such integer).\n",
        "\n",
        "high : int, optional\n",
        "If provided, one above the largest (signed) integer to be drawn from the distribution (see above for behavior if high=None).\n",
        "\n",
        "size : int or tuple of ints, optional\n",
        "Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn. Default is None, in which case a single value is returned.\n",
        "\n",
        "dtype : dtype, optional\n",
        "Desired dtype of the result. All dtypes are determined by their name, i.e., ‘int64’, ‘int’, etc, so byteorder is not available and a specific precision may have different C types depending on the platform. The default value is ‘np.int’.\n",
        "\n",
        "New in version 1.11.0.\n",
        "\n",
        "### Returns:\t\n",
        "out : int or ndarray of ints\n",
        "size-shaped array of random integers from the appropriate distribution, or a single such random int if size not provided.\n",
        "\n",
        "# numpy.random.randn\n",
        "\n",
        "Return a sample (or samples) from the “standard normal” distribution.\n",
        "\n",
        "## numpy.random.randn(d0, d1, ..., dn)\n",
        "\n",
        "If positive, int_like or int-convertible arguments are provided, randn generates an array of shape (d0, d1, ..., dn), filled with random floats sampled from a univariate “normal” (Gaussian) distribution of mean 0 and variance 1 (if any of the d_i are floats, they are first converted to integers by truncation). A single float randomly sampled from the distribution is returned if no argument is provided.\n",
        "\n",
        "This is a convenience function. If you want an interface that takes a tuple as the first argument, use numpy.random.standard_normal instead.\n",
        "\n",
        "### Parameters:\t\n",
        "d0, d1, …, dn : int, optional\n",
        "The dimensions of the returned array, should be all positive. If no argument is given a single Python float is returned.\n",
        "\n",
        "### Returns:\t\n",
        "Z : ndarray or float\n",
        "A (d0, d1, ..., dn)-shaped array of floating-point samples from the standard normal distribution, or a single such float if no parameters were supplied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSmXjplAnVmD",
        "colab_type": "code",
        "outputId": "5f958dcf-b31c-4348-9133-bf9f0fa9b9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
        "tic = time.process_time()\n",
        "dot = np.dot(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### VECTORIZED OUTER PRODUCT ###\n",
        "tic = time.process_time()\n",
        "outer = np.outer(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
        "tic = time.process_time()\n",
        "mul = np.multiply(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "### VECTORIZED GENERAL DOT PRODUCT ###\n",
        "tic = time.process_time()\n",
        "dot = np.dot(W,x1)\n",
        "toc = time.process_time()\n",
        "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dot = 278\n",
            " ----- Computation time = 0.30825900000008843ms\n",
            "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            " ----- Computation time = 0.4082429999998638ms\n",
            "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
            " ----- Computation time = 0.10317100000012402ms\n",
            "gdot = [24.58519261 18.52018066 19.99011029]\n",
            " ----- Computation time = 0.6403729999999719ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPP5BPcVneh1",
        "colab_type": "text"
      },
      "source": [
        "As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger. \n",
        "\n",
        "**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and the `*` operator (which is equivalent to  `.*` in Matlab/Octave), which performs an element-wise multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4LOoyYunkVr",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Implement the L1 and L2 loss functions\n",
        "\n",
        "**Exercise**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
        "\n",
        "**Reminder**:\n",
        "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
        "- L1 loss is defined as:\n",
        "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU8itL1hnZ5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: L1\n",
        "\n",
        "def L1(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- the value of the L1 loss function defined above\n",
        "    \"\"\"\n",
        "    return np.sum(abs(yhat-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtzU33NGn7Vu",
        "colab_type": "code",
        "outputId": "160dd50a-1a81-4566-83b3-2b903ee7688f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L1 = \" + str(L1(yhat,y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1 = 1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNrwHlxVoPyc",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
        "\n",
        "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RGY8EEuoJLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- the value of the L2 loss function defined above\n",
        "    \"\"\"\n",
        "    return np.sum(np.matmul(yhat-y,yhat-y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSk0UPLGoQ2G",
        "colab_type": "code",
        "outputId": "b31ac6ca-e837-4109-9687-780cb293a9cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L2 = \" + str(L2(yhat,y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 = 0.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJeT7Og2p1Cp",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>\\\n",
        "**What to remember:**\n",
        "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
        "- You have reviewed the L1 and L2 loss.\n",
        "- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...\n",
        "  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXu5rMlmpzCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}